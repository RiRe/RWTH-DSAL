\documentclass[12pt]{article}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}

\renewcommand*\familydefault{\sfdefault}

\usepackage[T1]{fontenc}
%\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx,colordvi,color}
\usepackage{csquotes}
\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

%Seitenraender
\usepackage{geometry}
\geometry{a4paper, top=35mm, left=20mm, right=20mm, bottom=35mm, headsep=20mm, footskip=12mm}

%Kopf- und Fusszeile
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{Zusammenfassung\\Datenstrukturen und Algorithmen}
\rhead{Richard Reiber\\ \today}
\cfoot{\thepage}
\usepackage{booktabs}
%\usepackage{pgfplots}
%\usepackage{tikz}
%\usepackage{pst-node}

%\usetikzlibrary{arrows,automata, positioning, shapes,snakes}
\begin{document}\parindent=0pt

\pagenumbering{roman}
\tableofcontents
\clearpage
\pagenumbering{arabic}

\section{Komplexität und Laufzeiten}

\subsection{Definitionen}
$D_n$: Menge aller Eingaben der Länge $n$

$t(I)$: Benötigte Anzahl elementarer Operationen für Eingabe $I$

$Pr(I)$: Wahrscheinlichkeit, dass Eingabe $I$ auftritt

\subsection{Worst-, Best- und Average-Case-Laufzeit}

W(n) = max $\{ t(I) | I \in D_n \}$

B(n) = min $\{ t(I) | I \in D_n \}$

A(n) = $\sum_{I \in D_{n}}$ $Pr(I) \cdot{} t(I)$

\subsection{Asymptotische Betrachtung}

Koeffizienten und Terme niedriger Ordnung werden ignoriert

\subsection{L'Hospital}

Seien f, g: $\mathbb{N} \to \mathbb{R}$ differenzierbare Funktionen mit \(\lim\limits_{n \to \infty}f(n)=\lim\limits_{n \to \infty}g(n)=\infty\)

Dann gilt: \(\lim\limits_{n \to \infty}\frac{g(n)}{f(n)}=\lim\limits_{n \to \infty}\frac{g'(n)}{f'(n)}\)

\subsection{Die Klasse Gross-$\mathcal O$}

$\mathcal O(f)$ ist die Menge von Funktionen, die \textbf{nicht schneller} als die Funktion $f$ wachsen.

Diese Eigenschaft gilt ab einer Konstanten $n_0$, Werte unter $n_0$ werden vernachlässigt.

$g \in \mathcal O(n^2)$ sagt mehr aus als $g \in \mathcal O(n^3)$.\\

\textbf{Beispiel}: $g \in \mathcal O(f)$ heißt, dass $c \cdot{} f(n)$ eine obere Schranke für $g(n)$ ist.\\

$\mathcal O(cf(n)) = \mathcal O(f(n))$

$\mathcal O(f(n)) + \mathcal O(g(n)) = \mathcal O(f(n) + g(n))$

$\mathcal O(f(n)) + \mathcal O(g(n)) = \mathcal O($max \{$f(n), g(n)$\}$)$

$\mathcal O(f(n))\mathcal O(g(n)) = \mathcal O(f(n)g(n))$

$f(\mathcal O(1)) = \mathcal O(1)$ (das ist jetzt weder formal korrekt noch irgendwie sinnvoll, aber es stand da so)\\

$\sum_{k=1}^n \mathcal O(f(k))$ $\subseteq \mathcal O(nf(n))$, wenn $f(n)$ monoton steigt

\subsection{Die Klasse Gross-Omega}

$\Omega(f)$ ist die Menge von Funktionen, die \textbf{nicht langsamer} als die Funktion $f$ wachsen.

Diese Eigenschaft gilt ab einer Konstanten $n_0$, Werte unter $n_0$ werden vernachlässigt.

$g \in \Omega(n^2)$ sagt mehr aus als $g \in \Omega(n)$.\\

\textbf{Beispiel}: $g \in \Omega(f)$ heißt, dass $c \cdot{} f(n)$ eine untere Schranke für $g(n)$ ist.\\

\subsection{Die Klasse Gross-Theta}

$\Theta(f)$ ist die Menge von Funktionen, die \textbf{genauso schnell} wie die Funktion $f$ wachsen.

Diese Eigenschaft gilt ab einer Konstanten $n_0$, Werte unter $n_0$ werden vernachlässigt.\\

\textbf{Beispiel}: $g \in \Theta(f)$ heißt, dass $c_1 \cdot{} f(n)$ eine untere Schranke und $c_2 \cdot{} f(n)$ eine obere Schranke für $g(n)$ ist.\\

$g \in \Theta(f)$ wenn \(\lim\limits_{n \to \infty}\frac{g(n)}{f(n)}=c\) für ein $0 < c < \infty$

$g \in \Theta(f)$ gdw. $g \in \mathcal O(f)$ und $g \in \Omega(f)$

\subsection{Die Klasse Klein-O und Klein-Omega}

$o(f)$ ist die Menge der Funktionen, die \textbf{echt langsamer} als $f$ wachsen.

$\omega(f)$ ist die Menge der Funktionen, die \textbf{echt schneller} als $f$ wachsen.\\

$f \in o(g)$ gdw. $g \in \omega(f)$

\subsection{Elementare Eigenschaften}

\textbf{Reflexivität}: $f \in \mathcal O(f)$, $f \in \Omega(f)$, $f \in \Theta(f)$

\textbf{Transitivität}: Aus $f \in \mathcal O/\Omega/\Theta(g)$ und $g \in O/\Omega/\Theta(h)$ folgt $f \in \mathcal O/\Omega/\Theta(h)$.

\textbf{Symmetrie von $\Theta$}: $f \in \Theta(g)$ gdw. $g \in \Theta(f)$

\textbf{Beziehung zwischen $\mathcal O$ und $\Omega$}: $f \in \mathcal O(g)$ gdw. $g \in \Omega(f)$

\subsection{Platzkomplexität}

Wir definieren $S(n)$ als Platzkomplexität.

Bei der Speicherung eines Liedes ist $S(n) \in \mathcal O(n)$ und $S(n) \in \Omega(1)$.

\section{Suchalgorithmen}

\subsection{Bilineare Suche}

Array wird aus beiden Richtungen \enquote{gleichzeitig} durchsucht.

Zeitkomplexität wie bei linearer Suche $\mathcal O(n)$.

\subsection{Binäre Suche}

Sortiertes Array $E$ mit $n$ Elementen, gesucht ist das Element $K$.

Wir beginnen in der Mitte und \textbf{halbieren} den Suchraum in jedem Durchlauf.

Zeitkomplexität ist $\mathcal O($log $n)$, allerdings ist die Initialisierung mit $\mathcal O(n $ log $ n)$ aufwändig.

\section{Rekursionsgleichungen}

\subsection{Mastertheorem}

$T(n) = b \cdot{} T(\frac{n}{c}) + f(n)$ mit $b \geq 1$ und $c > 1$

Anzahl der Blätter im Rekursionsbaum: $n^E$ mit $E = \frac{\log(b)}{\log(c)} = $ log$_c(b)$\\

\textbf{Fall 1}

Bedingung: $f(n) \in \mathcal O(n^{E-\epsilon})$ für ein $\epsilon > 0$

Es folgt: $T(n) \in \Theta(n^E)$\\

\textbf{Fall 2}

Bedingung: $f(n) \in \Theta(n^{E})$

Es folgt: $T(n) \in \Theta(n^E$ $\cdot{}$ log $n)$\\

\textbf{Fall 3}

1. Bedingung: $f(n) \in \Omega(n^{E+\epsilon})$ für ein $\epsilon > 0$

2. Bedingung: $b\cdot{}f(\frac{n}{c}) \leq d \cdot{} f(n)$ für ein $d < 1$ und ein hinreichend großes $n$

Es folgt: $T(n) \in \Theta(f(n))$

\section{Sortieren}

\subsection{Grundlegendes}

Ein Sortieralgorithmus ist \textbf{stabil}, wenn er die Reihenfolge der Elemente, deren Sortierschlüssel gleich sind, aufrecht erhält. Wir bezeichnen einen Sortieralgorithmus als \textbf{in-place}, wenn er außer dem für die Speicherung der zu bearbeitenden Daten benötigten Speicher nur eine konstante Menge von Speicher benötigt (die also von der zu bearbeitenden Datenmenge unabhängig ist).

\subsection{Insertionsort}

Unsortierte Elemente werden in sortiertes Array vor dem nächstgrößeren Element eingefügt.

Insertionsort ist im Worst- und Average-Case quadratisch, im Best-Case linear.

\subsection{Divide-and-Conquer-Algorithmen}

Teile-und-Beherrsche Algorithmen (divide-and-conquer) teilen das Problem in mehrere Teilprobleme auf, die dem Ausgangsproblem ähneln, jedoch von kleinerer Größe sind.

Sie lösen die Teilprobleme rekursiv und kombinieren diese Lösungen dann, um die Lösung des eigentlichen Problems zu erstellen.

\subsection{Mergesort}

Array wird in zwei möglichst gleich große Hälften geteilt, die durch rekursive Mergesort-Aufrufe sortiert werden.

Es ist $W(n) = B(n) = A(n) \in \Theta(n \cdot{} \log(n))$.

Speicherbedarf $\Theta(n)$ für die Kopie des Arrays und $\Theta(\log(n))$ für die Verwaltung der Rekursion.

\subsection{Heaps}

Ein Heap ist ein Binärbaum, der Elemente mit Schlüsseln enthält. Für \textbf{Max-Heaps} gelten folgende Bedingungen:

\begin{itemize}  
\item Der Schlüssel eines Knotens ist stets mindestens so groß wie die Schlüssel seiner Kinder.
\item Alle Ebenen (mit Ausnahme der untersten) sind komplett gefüllt.
\item Die Blätter befinden sich damit auf einer (oder zwei) Ebene(n).
\item Die Blätter der untersten Ebene sind linksbündig angeordnet.
\end{itemize}

Heaps lassen sich in Arrays einbetten, dabei liegt die Wurzel in $a[0]$, das linke Kind von $a[i]$ in $a[2\cdot{}i+1]$ und das rechte Kind in $a[2\cdot{}i+2]$.

Die Höhe eines Heaps in Abhängigkeit von seinen Elementen ist gegeben durch $H = \lfloor{}log_2(n)\rfloor$. Es ist $H \in \mathcal O($log $n)$.

Für die Anzahl der Knoten im linken Teilbaum $l$ und die Knotenzahl im rechten Teilbaum $r$ gilt stets: $r \leq l \leq 2r + 1$.

Ein Heap wird Reihe für Reihe von oben nach unten aufgebaut, indem neue Elemente möglichst weit links angefügt werden. Ist ein neues Element größer als sein Elternknoten, wird rekursiv nach oben getauscht.

\subsection{Heapsort}

Im Max-Heap enthält die Wurzel immer die größte Ziffer, deshalb tauschen wir die Wurzel mit dem letzten Element des Heaps, das in der untersten Ebene ganz rechts steht. Nun ist die letzte Position im Heap sortiert, wir können den Heap nun als Array aufschreiben und haben die letzte Position sortiert.

Dieses Verfahren wird rekursiv angewendet, indem das letzte, sortierte Element gestrichen wird. Anschließend muss wieder ein Max-Heap aufgebaut werden und wir beginnen von vorne, bis im Heap keine Elemente mehr enthalten sind, sodass das Array sortiert ist.\\

Auch Heapsort hat eine Laufzeitkomplexität von $\mathcal O(n\cdot{}\log(n))$, der Speicherplatzbedarf ist konstant. Heapsort ist ein Inplace-Sortieralgorithmus und nicht stabil.

\subsection{Prioritätswarteschlange}

Elemente werden mit Schlüsseln versehen, wobei jeder Schlüssel höchstens an ein Element vergeben wird. Die Schlüssel betrachten wir nun als Priorität. Prioritätswarteschlangen können mit einem Array, einem sortierten Array oder einem Heap implementiert werden.

\subsection{Quicksort}

Wir wählen ein beliebigies Pivotelement und teilen das Array danach in zwei Teile - der linke Teil enthält Elemente, die kleiner als das Pivotelement sind, der rechte Teil enthält Elemente, die mindestens so groß sind wie das Pivotelement, allerdings nicht das Pivotelement selbst. Das wiederholen wir rekursiv. Die Pivotelemente sind am Ende in korrekter Reihenfolge.\\

Die Platzkomplexität von Quicksort ist $\Theta(\log(n))$, es ist $W(n) \in \Theta(n^2)$, $B(n) \in \mathcal O(n\cdot{}\log(n))$ und $A(n) \in \mathcal O(n\cdot{}\log(n))$.

\subsection{Bubblesort}

Es werden alle nebeneinanderstehenden Paare in der Liste vertauscht, die momentan in der falschen Ordnung stehen. Die Liste rechts von der letzten Tauschposition des letzten Durchgangs ist dabei stets bereits sortiert. Sobald keine Änderungen mehr auftreten, ist der Algorithmus fertig - ansonsten muss der Algorithmus erneut durchlaufen.\\

Damit ist $W(n) \in \mathcal O(n^2)$.

\subsection{Countingsort}

Die Elemente des Array sind aus dem Bereich \{0, ..., $B$\} für einen bekannten Wert $B$. Wir erstellen ein Counting-Array, das so viele Positionen wie annehmbare Werte hat - jedem annehmbaren Wert wird (fortlaufend) eine Position im Counting-Array zugeordnet. Dieses Array wird mit Nullen initialisiert.

Nun gehen wir das zu sortierende Array durch. Für jedes Element im Array wird der zum Element gehörende Wert im Counting-Wert um 1 inkrementiert. Nun ersetzen wir im Counting-Array die Elemente durch die jeweiligen Teilsummen ($c[0] = c[0]$, $c[1] = c[0] + c[1]$, ...).

Abschließend gehen wir das unsortierte Array von hinten nach vorne durch. Bei jedem Element des unsortierten Arrays steht nun die neue, sortierte Position des Elements im Counting-Array. Bei jedem Einfügen eines Elements muss der zu diesem Element gehörige Wert im Counting-Array um 1 dekrementiert werden.

Die Laufzeit von Countingsort ist $\in \mathcal O(n+B)$, der Speicherbedarf $\in \mathcal O(n+B)$.

\section{Elementare Datenstrukturen}

\subsection{Abstrakter Datentyp (ADT)}

Ein abstrakter Datentyp besteht aus einer Datenstruktur (Menge von Werten) und einer Menge von Operationen darauf, zum Beispiel aus Konstruktor, Zugriffsfunktionen und Bearbeitungsfunktionen. Damit sind die Daten außerhalb des ADT nur über wohldefinierte Operationen zugänglich (Datenkapselung), während die Repräsentation der Daten nur für die Implementierung des ADT relevant ist.

Es gibt zum Beispiel \textit{Stack}, \textit{Queue}, \textit{Priority-Queue} und einfach bzw. doppelt verkettete Listen.

\subsection{Binärbaum}

Binärbäume enthalten Knoten mit einem linken und einem rechten Nachfolger (die jeweils leer sein können). Es gibt eine Wurzel (ganz oben), innere Knoten mit mindestens einem Nachfolger und Blätter ohne Nachfolger.

Die Tiefe eines Knotens ist der Abstand des Knoten zur Wurzel, die Höhe eines Baumes ist die (maximale) Tiefe des untersten Knotens. Eine Ebene $d$ enthält maximal $2^d$ Knoten, ein Binärbaum der Höhe $h$ enthält höchstens $2^{h+1}-1$ Knoten. Ein Binärbaum ist vollständig, wenn die möglichen Knotenzahl für die Höhe des Baumes voll ausgefüllt ist.

Bei einer Traversierung werden alle Elemente des Baumes einmal besucht, jede Kante wird dabei zweimal durchlaufen. Die Traversierung beginnt und endet an der Wurzel des Baumes. Die Teilbäume der Knoten werden in einer festen Reihenfolge (zuerst linker, dann rechter Teilbaum) besucht.

Es gibt verscheidene Traversierungen, die sich alleine dadurch unterscheiden, wann sie die Elemente des Knotens \enquote{besuchen}. So gibt es Inorder-, Preorder- und Postorder-Traversierung.

\subsection{Binäre Suchbäume}

Ein binärer Suchbaum (BST) ist ein Binärbaum, der Elemente mit Schlüsseln enthält, wobei der Schlüssel jedes Knotens mindestens so groß ist wie jeder Schlüssel im linken Teilbaum und höchstens so groß ist wie jeder Schlüssel im rechten Teilbaum.

Im binären Suchbaum gibt es neben dem Element des Knotens und einem Verweis auf die Nachfolger auch noch einen Verweis auf den Vaterknoten - dieser ist bei der Wurzel leer.

Die Inorder-Traversierung gibt aufgrund des Aufbaus des Suchbaums alle Schlüssel des Baumes in sortierter Reihenfolge aus.

Kompliziert ist im BST vor allem das Löschen, wenn das zu löschende Element in einem Knoten mit zwei Kindern steht (bei einem Kind wird der Knoten entfernt und der Zeiger vom Vaterknoten auf das Kind gelegt, ein Blatt wird einfach entfernt). Bei zwei Kindern finden wir den Nachfolger des zu löschenden Knotens, entfernen ihn aus seiner ursprünglichen Position und ersetzen den zu löschenden Knoten mit ihm.

Alle Operationen auf einem BST sind linear und $\in \Theta(h)$.

\subsection{Rotationen in Bäumen}

Bei einer Linksrotation rückt der rechte Nachfolger (N) des Knotens (K) an die Position von K. K wird linker Nachfolger von N, der ehemalige linke Nachfolger von N wird rechter Nachfolger von K. Die Rechtsrotation verläuft analog.

Durch die Rotation bleibt ein BST weiterhin ein BST, auch die Inorder-Traversierung bleibt unverändert. Die Zeitkomplexität der Rotation ist $\in \Theta(1)$.

\subsection{AVL-Bäume}

Ein AVL-Baum ist ein balanzierter BST, bei dem sich für jeden Knoten die Höhen der beiden Teilbäume um höchstens 1 unterscheiden - die Höhe der Teilbäume wird balanziert. Es gibt ein zusätzliches Datenfeld für jeden Knoten, in dem über die Höhe des Unterbaums Buch geführt wird.

Nach jeder Operation muss die Bilanz des AVL-Baums wiederhergestellt werden, dies ist in $\Theta(h)$ möglich. Dadurch ist stets $h \in \Theta(\log(n))$ gewährleistet, $\Theta(\log(n))$ kann für alle Operationen auf dem Baum garantiert werden.

Es kann - wie bereits angerissen - passieren, dass der AVL-Baum nach dem Einfügen eines Knotens unbalanziert ist und die Bedingungen damit verletzt sind. Dann muss der  Baum neu balanziert werden. Dafür gibt es folgende Regeln:

\begin{itemize}  
\item Wenn das neue Blatt außen am Baum hängt, reicht eine einfache Rotation nach innen um den Vaterknoten.
\item Wenn das neue Blatt innen am Baum hängt, muss eine Doppelrotation durchgeführt werden. Dazu rotieren wir erst um den Vaterknoten nach außen, sodass sich dann eine Situation wie im ersten Fall ergibt. Es wird nun um den neuen Vaterknoten nach innen rotiert.
\end{itemize}

Auch durch Löschen (von Blättern) kann der AVL-Baum unbalanziert werden, dann muss ebenfalls durch Rotieren Abhilfe geschaffen werden.

\subsection{Rot-Schwarz-Bäume (RBT)}

In Rot-Schwarz-Bäumen gibt es zwei Arten von Knoten: Schwarze Knoten werden strikt balanziert, während rote Knoten nur als \enquote{Schlupf} dienen. Dabei muss die Anzahl der Schlupfknoten beschränkt bleiben. Die Balanzierungsanforderungen an die AVL-Bäume werden damit aufgeweicht, außerdem reduziert sich der benötigte Speicherplatz für zusätzliche Informationen auf 1 Bit (die Farbe), da der Balanzierungsfaktor nicht mehr gespeichert wird.

Die Wurzel eines RBT ist immer schwarz - jeder Knoten muss entweder rot oder schwarz sein. Außerdem darf jeder rote Knoten nur schwarze Kinder haben. Externe Knoten (Null-Zeiger, also quasi nicht existente Knoten) sind immer schwarz. Für jeden Knoten müssen alle Pfade, die an diesem Knoten starten und in einem externen Knoten enden, die gleiche Anzahl schwarzer Knoten enthalten. In Zeichnungen werden die externen Knoten häufig weggelassen.

Die Schwarzhöhe $bh(x)$ eines Knotens $x$ ist die Anzahl schwarzer Knoten bis zu einem (externen) Blatt ($x$ zählt nicht mit, das Blatt/der externe Knoten schon), die Schwarzhöhe $bh(t)$ eines RBT $t$ ist die Schwarzhöhe seiner Wurzel.

Ein Rot-Schwarz-Baum $t$ mit der Schwarzhöhe $h = bh(t)$ mindestens $2^h-1$ und höchstens $4^h-1$ innere Knoten - das sind alle \enquote{internen} Knoten.

Neue Knoten werden immer als rote Knoten eingefügt, weil das Einfügen von schwarzen Knoten im Regelfall zur Verletzung der Schwarz-Höhen-Bedingung führt. Die Position für das Einfügen suchen wir uns wie in einem BST. Allerdings kann durch das Einfügen eines roten Knotens eine Rot-Rot-Verletzung auftreten, wenn bereits der Vater des neu einzufügenden Knotens rot ist:

\begin{enumerate}
\item Wenn der Onkel des Vaters ebenfalls rot ist, können wir den Vater und den Onkel schwarz sowie den Großvater rot färben, um die Rot-Rot-Verletzung zu beheben. Eventuell entstehen dadurch überhalb des Vaters erneut Rot-Rot-Verletzungen die iterativ genauso behoben werden müssen. Sofern die Wurzel durch der Vertauschung rot gefärbt wurde, können wir diese einfach wieder schwarz färben - die Schwarz-Höhe des Baumes erhöht sich dadurch um eins, eine Schwarz-Höhen-Verletzung tritt dadurch aber nicht auf.
\item Wenn der Onkel des Vaters nicht rot ist und das neue Blatt im Baum innen hängt, müssen wir über den Vater nach außen rotieren. Dann haben wir Fall 3.
\item Wenn der Onkel des Vaters nicht rot ist und das neue Blatt im Baum außen hängt, muss zunächst nach rechts um den Großvater rotiert werden. Sodann wird der ehemalige Großvater rot gefärbt, damit die Schwarz-Höhe von $(x+1)$ wieder auf $x$ korrigiert wird. Der neue Vater wird dann schwarz gefärbt.
\end{enumerate}

Auch das Löschen von schwarzen Knoten in einem RBT ist problematisch, weil dadurch in der Regel eine Schwarz-Höhen-Verletzung entsteht - hingegen ist das Löschen von roten Knoten unproblematisch. Auch hier gibt es mehrere Fälle:

\begin{enumerate}
\item Hat der zu löschende Knoten zwei Kinder, suchen wir seinen Nachfolger und entfernen ihn aus dem Rot-Schwarz-Baum. Eine eventuell neu auftretende Farbverletzung ist dabei zu beheben. Nun ersetzen wir den zu löschenden Knoten durch seinen Nachfolger und übernehmen die Farbe des Nachfolgers.
\item Hat der zu löschende Knoten nur ein Kind, kann der Knoten gelöscht werden. Anschließend kann die Farbverletzung durch einfaches Vertauschen der Farben oder die \enquote{Weitergabe} des Schwarzwertes Richtung Wurzel behoben werden.
\item Hat der zu löschende Knoten keine Kinder, muss der Onkel und der Vater unter Beachtung der Schwarzhöhe umgefärbt werden. Die eventuell dadurch entstehenden Farbverletzungen müssen iterativ behoben werden.
\end{enumerate}

Die Laufzeit der Algorithmen für Einfügen und Löschen liegt in $\Theta(\log(n))$, alle anderen Operationen sind mit denen in einem BST algorithmisch und bezüglich der Laufzeit identisch.

\section{Hashing}

\subsection{Dictionary}

Ein \textit{Dictonary} (auch \textit{assoziatives Array)} speichert Daten, die jederzeit anhand ihres Schlüssels abgerufen werden können. Die Daten sind dabei dynamisch gespeichert.

\subsection{Direkte Adressierung}

Bei der direkten Adressierung gibt es ein Array (die Direkte-Adressierungs-Tabelle) so, dass es für jeden möglichen Schlüssel eine entsprechende Position gibt. Jedes Array-Element enthält dabei einen Pointer auf die gespeicherte Information.

\subsection{Grundlagen des Hashings}

Ein Hash ist ein Algorithmus, der aus einer großen Datenmenge eine sehr kleine Zusammenfassung (Identifikation; Fingerabdruck) berechnet. Dabei ist er klar definiert und einfach sowie ressourcenschonend zu berechnen.

Das Ziel von Hashing ist es, einen extrem großen Schlüsselraum auf einen vernünftigen, kleinen Bereich von ganzen Zahlen abzubilden. Dabei soll es möglichst unwahrscheinlich sein, dass zwei Schlüssel auf die gleiche Zahl abgebildet werden. In der Praxis wird hierbei oft nur ein kleiner Teil der Schlüssel verwendet, damit ist bei der direkten Adressierung ein Großteil des Arrays verschwendet.

$h(k)$ sei der Hashwert von $k$. Das Auftreten von $h(k) = h(k')$ für $k \neq k'$ nennt man Kollision.

\subsection{Hashfunktionen}

Eine gute Hash-Funktion ist einfach zu berechnen, surjektiv auf der Zielmenge und verwendet alle Indizes mit möglichst gleicher Häufigkeit. Ferner sollen ähnliche Schlüssel möglichst breit auf die Hashtabelle verteilt werden. Es gibt einige Basistechniken, um eine gute Hashfunktion zu erhalten:\\

Bei der Divisionsmethode wird die Hashfunktion mit $h(k) = k$ mod $m$ gebildet. Dabei muss man $m$ sorgfältig wählen, da zum Beispiel bei $m = 2^p$ einfach die letzten $p$ Bits genommen werden. Eine gute Wahl ist $m$ prim und nicht zu nah an einer Zweierpotenz.\\

Die Multiplikationsmethode stellt $h(k) = \lfloor{}m \cdot{} (k \cdot{} c$ mod $1)\rfloor$ mit $0 < c < 1$. Der Parameter $m$ ist dabei nicht kritisch, Knuth empfiehlt $c = \frac{(\sqrt{5} - 1)}{2} \approx 0,618$.

\subsection{Universelles Hashing}

Das größte Problem beim Hashing ist, dass es immer eine ungünstige Sequenz von Schlüsseln gibt, die auf den gleichen Hashwert abgebildet werden. Die Idee zur Lösung dieses Problem ist, zufällig eine Hashfunktionen aus einer gegebenen, kleinen Menge $H$ auszuwählen - unabhängig von den verwendeten Schlüsseln.

\subsection{MD5}

MD5 übersetzt Nachrichten beliebiger Länge in einen 128-Bit-Hashwert. Inzwischen gilt MD5 als sehr unsicher, Kollisionen sind verhältnismäßig einfach zu berechnen.

\subsection{Verkettung}

Bei der Verkettung wird in der Hash-Tabelle an der Stelle des gehashten Schlüssels nicht nur der Wert gespeichert, sondern auch der Original-Schlüssel (vor dem Hashen). Die Speicherung erfolgt in der Hash-Tabelle als verkettete Liste. Dadurch erhöht sich natürlich den Speicheraufwand, außerdem muss beim Speichern und Abrufen eine Suche durchgeführt werden.

\subsection{Offene Adressierung}

Bei der offenen Adressierung werden alle Schlüssel direkt an einer Position in der Hashtabelle gespeichert (im Gegensatz zur Verkettung). Wenn ein Schlüssel eingefügt werden soll, wird geprüft, ob die entsprechende Position in der Hashtabelle frei ist. Ist dies nicht der Fall, kann der Schlüssel nicht (wie bei der Verkettung) einfach angehangen werden. Es muss dann - in Abhängigkeit des einzufügenden Schlüssels - nach einer freien Position gesucht werden.

Damit das funktioniert, bekommt die Hashfunktion $h(k)$ nun einen zweiten Parameter, der in $[0,m-1]$ ($m$ ist die Größe der Hashtabelle) liegt. Dieser Parameter gibt die Nummer der Sondierung (das ist die Nummer des Versuchs, eine freie Position zu finden) an. Die Hashfunktion liefert nun in der Regel eine andere Position, die wir in der Hashtabelle prüfen. Sobald wir eine leere Position finden, wird der Schlüssel dort in der Hashtabelle eingefügt und wir sind fertig. Finden wir keine leere Position, führen wir das Verfahren iterativ durch, erhöhen also den zweiten Parameter der Hashhfunktion in jedem Durchlauf.

Über den eindeutigen Index des Schlüssels in der Hashtabelle kann man dann mithilfe einer Direkten-Adressierungs-Tabelle auf das eigentlich assoziativ zu speichernde Element verweisen.

Die Suche in der Hashtabelle nach einem Schlüssel verläuft ähnlich - wir gehen wieder nach dem Schema durch die Tabelle und sind fertig, sobald wird den gesuchten Schlüssel gefunden haben. Es wird also nicht geprüft, ob die jeweilige Position leer ist, sondern ob der Inhalt der Hashtabelle an dieser Position dem gesuchten Schlüssel entspricht.

Aufpassen müssen wir beim Löschen eines Elements. Man könnte einfach wieder die Position des Elements suchen und dann die Hashtabelle an dieser Stelle leeren. Das führt aber zu dem Problem, dass alle eventuell nach diesem Element eingefügte Elemente nicht mehr gefunden werden, da die Sondierung früher abgebrochen wird. Daher ersetzen wir das Element beim Löschen durch z.B. "DELETED". Dieser Wert wird bei der Sondierung als leer interpretiert - beim Einfügen wird der Slot wiederverwendet, bei der Suche übersprungen.

Dadurch sind die Suchzeiten nicht mehr allein vom Füllgrad der Hashtabelle abhängig, deswegen verwendet man zur Kollisionsauflösung meist Verkettung, wenn Schlüssel auch gelöscht werden sollen.

\subsection{Sondierung}

Wir gehen nun näher auf das Verfahren der Sondierung ein. Für jeden Schlüssel $k$ benötigen wir eine Sondierungssequenz $\langle{}h(k,0), h(k,1), ..., h(k, m-1)\rangle$, um die offene Adressierung nutzen zu können. Dabei sollte die Sondierungssequenz eine Permutation von $\langle{}0, ..., m-1\rangle$ sein, damit jeder Slot auch wirklich verwendet wird. Ideal wäre gleichverteiltes Hashing, sodass jede der $m!$ Permutationen als Sondierungssequenz gleich wahrscheinlich ist. In der Praxis ist das aber zu aufwändig, daher wird approximiert.\\

\textbf{Lineares Sondieren}

$h(k, i) = (h'(k) + i)$ mod $m$ ist eine Hashfunktion für lineares Sondieren. Dabei ist $h'$ eine normale Hashfunktion (siehe 6.4). Die Verschiebung der nachfolgenden Sondierungen hängt damit linear von $i$ ab. Damit wird die gesamte Sequenz bereits durch die erste Sondierung bestimmt und es gibt nur $m$ verschiedene Sequenzen.

Das führt zum Problem des Clusterings: Lange Folgen von belegten Slots tendieren dazu, noch länger zu werden, weil $h'(k)$ konstant bleibt und nur der Offset jedes Mal um eins größer wird.\\

\textbf{Quadratisches Sondieren}

$h(k, i) = (h'(k) + c_1 \cdot{} i + c_2 \cdot{} i^2)$ mod $m$ ist eine Hashfunktion für quadratisches Sondieren. Dabei ist $h'$ eine normale Hashfunktion (siehe 6.4) und $c_1, c_2 \in \mathbb{N}$ sind geeignete Konstanten. Die Verschiebung der nachfolgenden Sondierungen hängt damit quadratisch von $i$ ab. Damit wird die gesamte Sequenz auch bei der quadratischen Sondierung bereits durch die erste Sondierung bestimmt und es gibt nur $m$ verschiedene Sequenzen.

Das Problem des Clusterings wird vermieden oder zumindest abgeschwächt. Allerdings tritt immer noch sekundäres Clustering auf, denn $h(k,0) = h(k',0)$ verursacht $h(k,i) = h(k',i)$ für alle $i$.\\

\textbf{Doppeltes Hashing}

$h(k, i) = (h_1(k) + i \cdot{} h_2(k))$ mod $m$ ist eine Hashfunktion für doppeltes Hashing. Dabei sind $h_1$ und $h_2$ normale Hashfunktionen (siehe 6.4). Die Verschiebung der nachfolgenden Sondierungen hängt damit nur von $h_2$ ab. Damit wird die gesamte Sequenz nicht bereits durch die erste Sondierung bestimmt. Deswegen werden die Schlüssel besser in der Hashtabelle verteilt und es wird eine Approximierung von gleichverteiltem Hashing erzielt.

Sind $h_2$ und $m$ relativ prim, wird die gesamte Hashtabelle abgesucht. Man kann zum Beispiel $m = 2^k$ wählen und $h_2$ nur ungerade Zahlen erzeugen lassen, um ein gutes Ergebnis zu erzielen. Dann können $m^2$ verschiedene Permutationen erzeugt werden, weil jedes mögliche Paar von $h_1(k)$ und $h_2(k)$ eine andere Sequenz erzeugt.

\section{Graphalgorithmen}

\subsection{Graphen}

Graphen haben wir in der Vorlesung \textit{Diskrete Strukturen} definiert, diese bestehen aus dem Paar $(V, E)$ (Ecken/Knoten und Kanten). Die Kanten können gerichtet sein (eine Richtung haben) oder nicht - wir haben dann einen gerichteten bzw. ungerichteten Graphen. Einen gerichteten Graphen nennt man auch Digraph.

Der Knoten $u$ ist adjazent zu Knoten $v$ gdw. $(u,v) \in E$ und inzident zu der Kante $e = (u,v)$. Der Grad $deg(u)$ ist die Anzahl der zu $u$ inzidenten Kanten. Wir definieren - insbesondere für gerichtete Graphen - ferner den In-Grad $deg^+(u)$ und den Aus-Grad $deg^-(u)$. Ein Graph ist $k$-regulär, wenn alle Knoten den Grad $k$ haben.

In ungerichteten Graphen $G = (V, E)$ gilt das Handshake-Lemma $\sum_{v \in V}$ $deg(v) = 2\cdot{}|E|$, bei gerichteten Graphen ist $\sum_{v \in V}$ $deg^+(v) = \sum_{v \in V}$ $deg^-(v)$.

Ein Teilgraph eines Graphen $G = (V, E)$ ist ein Graph $G' = (V', E')$ mit $V' \subseteq V$ und $E' \subseteq E \cap (V' \times V')$. Ist $V' \subset V$ und/oder $E' \subset E$, so nennen wir $G'$ einen echten Teilgraphen von $G$. Wenn $E' = E \cap (V' \times V')$ ist, so ist $G'$ der durch $V'$ induzierte Teilgraph.

Ein gerichteter Graph $G$ heißt symmetrisch, wenn aus $(v,w) \in E$ immer auch $(w,v) \in E$ folgt. Zu jedem ungerichteten Graphen gibt es einen korrespondierenden symmetrischen, gerichteten Graphen.

Der Graph $G$ heißt vollständig, wenn jedes Paar von Knoten mit einer Kante verbunden ist. Es ist $K_n$ der ungerichtete, vollständige Graph mit $n$ Knoten.

Ferner können wir einen Graphen \textbf{transponieren}, der transponierte Graph von $G = (V, E)$ ist gegeben durch $G^T = (V, E')$ mit $(v, w) \in E'$ gdw. $(w, v) \in E$.

\subsection{Pfade und Kreise}

Ein Spaziergang (walk) von einem Knoten $v$ zu einem Knoten $w$ ist eine Folge $v_0, v_1, ..., v_k$ mit dem Startknoten $v_0 = v$ und dem Endknoten $v_k = w$. Ferner muss $(v_i, v_{i+1})$ für $0 \leq i \leq k-1$ eine Kante des Graphen sein.

Hat der Spaziergang nur paarweise verschiedene Knoten (d.h. $v_i \neq v_j$ für $i \neq j$), nennt man ihn simpel/einfach oder Pfad. Die Länge eines Spaziergangs ist die Anzahl der durchlaufenen Kanten.

Ein Kreis ist eine Spezialisierung des Pfades, dieser darf nicht-leer sein und es muss $v_0 = v_k$ sein - der Spaziergang muss also an der Stelle wieder enden, an der er begonnen hat, wir machen also eine Rundwanderung. Ein Graph ist kreisfrei oder azyklisch, wenn er keine Kreise als Teilgraphen hat.

Ein Knoten $w$ ist vom Knoten $v$ aus erreichbar, wenn es einen Pfad von $v$ nach $w$ gibt. Ein ungerichteter Graph $G$ ist zusammenhängend, wenn jeder Knoten von jedem anderen Knoten aus erreichbar ist. Eine Zusammenhangskomponente (connected component) von $G$ ist ein maximaler zusammenhängender Teilgraph von $G$.

In einem gerichteten Graphen $G'$ differenzieren wir genauer. $G'$ ist stark zusammenhängend (strongly connected), wenn jeder Knoten von jedem anderen aus erreichbar ist. Hingegen ist $G'$ schwach zusammenhängend, wenn der zugrunde liegende ungerichtete Graph (in dem alle Kanten ungerichtet gemacht worden sind) zusammenhängend ist. Eine starke Zusammenhangskomponente von $G'$ ist ein maximaler stark zusammenhängender Teilgraph von $G'$.

Jeder ungerichtete Graph kann eindeutig in Zusammenhangskomponenten, jeder gerichtete Graph eindeutig in starke Zusammenhangskomponenten aufgeteilt werden.

Ein Baum ist ein kreisfreier und zusammenhängender Graph. Hat ein Baum $n$ Knoten, so hat er $n-1$ Kanten. Ein ungerichteter Graph mit $n$ Knoten und $n-2$ oder weniger Kanten kann nicht zusammenhängend sein, während ein ungerichteter Graph mit $n$ Knoten und $n$ oder mehr Kanten einen Zyklus enthalten muss - in beiden Fällen kann dieser Graph also kein Baum sein.

\subsection{Repräsentation von Graphen}

Es stellt sich die Frage, wie man einen Graphen als Datenstruktur am sinnvollsten repräsentiert. Hierzu gibt es verschiedene Möglichkeiten, wir machen uns die Adjazenz zu Nutzen.\\

\textbf{Adjazenzmatrix}

Wir haben einen Graphen $G = (V, E)$ mit $|V| = n$, $|E| = m$ und $V = {v_1, ..., v_n}$. Dann ist der Graph in der Adjazenzmatrix-Darstellung gegeben durch eine $n \times n$-Matrix $A$ mit $A(i, j) = 1$ wenn $(v_i, v_j) \in E$ (und $0$ wenn nicht). Wenn $G$ ungerichtet ist, ist $A$ symmetrisch ($A^{tr} = A$), dann muss nur die Hälfte der Matrix gespeichert werden. Der Platzbedarf ist $\Theta(n^2)$.\\

\textbf{Adjazenzliste}

Wir können einen Graphen auch als Array von Adjazenzlisten darstellen. Dabei ist die Arrayposition gegeben durch die Nummer des Knotens $i$, an dieser Stelle enthält das Array eine verkettete Liste mit allen Kanten mit Startknoten $v_i$. Wenn G ungerichtet ist, wird jede Kante zweimal gespeichert. Der Platzbedarf ist $\Theta(n+m)$.

\subsection{Graphendurchlauf}

Viele Algorithmen untersuchen jeden einzelnen Knoten und jede einzelne Kante eines Graphen. Dazu gibt es verschiedene Graphendurchlaufstrategien, die jeden Knoten (oder jede Kante) genau einmal besuchen: Die Breitensuche (BFS) und die Tiefensuche (DFS). Dabei handelt es sich im Wesentlichen um Verallgemeinerungen der Strategien zur Baumtraversierung.

Allerdings müssen wir uns alle bereits besuchten Knoten explizit merken. Wir arbeiten mit der Adjazenzlisten-Darstellung, Algorithmen auf dieser Basis kosten $\mathcal O(|V| + |E|)$ Zeit.\\

\textbf{Breitensuche}

Bei der Breitensuche (Breadth-First Search, BFS) sind anfangs alle Knoten als unbesucht (WHITE) markiert. Es wird in einem beliebigen Knoten $v$ begonnen. Der aktuelle Knoten wird immer als aktiv (GRAY) markiert. Sodann wird für jede Kante $(v, w)$ mit unbesuchtem Nachfolger $w$ von allen Kanten dieses Nachfolgers aus weitergesucht. Insbesondere gibt es keinerlei Backtracking. Der Knoten $v$ wird dann als besucht (BLACK) markiert. Damit sind die schwarzen Knoten dann genau jene Knoten, die vom Startknoten aus erreichbar sind.

Für einen Knoten $v \in V$ bezeichnet $d(v)$ den Abstand vom Startknoten zum Knoten $v$, also die Anzahl der Kanten auf dem kürzesten Weg vom Startknoten nach $v$. Wenn ein Knoten $w$ in die Queue gegeben wird, so färben wir die dafür verantwortliche Kante $(v, w)$ gelb. Dann ist $v$ der Vater vom Knoten $w$.

BFS besucht die Knoten also in einer Reihenfolge mit ansteigendem Abstand vom Startknoten. Erst wenn alle Knoten mit Abstand $d$ verarbeitet sind, werden die Knoten mit Abstand $d+1$ angegangen. Die Suche terminiert, wenn für einen Abstand $d$ keine Knoten auftreten. Die zu verarbeitenden Knoten werden als FIFO-Queue organisiert, es gibt nur eine einzige Verarbeitungsmöglichkeit für $v$ (wenn es aus der Queue entnommen wird). Die gelben Kanten induzieren den Breitensuchbaum, der den Startknoten als Wurzel hat.

Die Zeitkomplexität der BFS ist $\mathcal O(|V| + |E|)$, der Platzbedarf ist $\Theta(|V|)$.\\

\textbf{Tiefensuche}

Auch bei der Tiefensuche (Depth-First Search, DFS) markieren wir zunächst alle Knoten als unbesucht (WHITE). Der aktuelle Knoten $v$ wird immer als aktiv (GRAY) markiert, es ist beliebig, mit welchem Knoten wir beginnen. Für jede Kante $(v, w)$ mit unbesuchtem Nachfolger $w$ suchen wir nun rekursiv von $w$ aus. Das bedeutet also, dass wir den neu entdeckten Knoten $w$ besuchen und von dort aus forschen, bis es nicht mehr weiter geht. Sodann gehen wir von $w$ nach $v$ zurück. Für jede Kante $(v, w)$, für die der Nachfolger $w$ schon besucht wurde, überprüfen wir die Kante, besuchen $w$ aber nicht. Abschließend markieren wir den Knoten $v$ als besucht (BLACK). Damit sind auch bei der DFS die schwarzen Knoten diejenigen, die vom Startknoten aus erreichbar sind.\\

Die Tiefensuche erforscht also einen Pfad so weit wie möglich und setzt dann zurück. Die zu verarbeitenden Knoten werden also in LIFO-Reihenfolge verarbeitet. Es gibt zwei mögliche Verarbeitungszeitpunkte für jeden Knoten - wenn dieser entdeckt oder wenn er abgeschlossen wird.\\

Bei der Tiefensuche gibt es verschiedene Klassen von Kanten. Baum-Kanten treten im DFS-Baum auf. Rückwärts-Kanten gehen von Knoten $u$ zum Vorfahren $v$, Vorwärts-Kanten gehen von einem Vorfahren $v$ zu einem Knoten $u$. Alle restlichen Kanten sind Quer-Kanten. In einem ungerichteten Graphen ist jede Kante entweder Baum-Kante oder Rückwärts-Kante.\\

Die Zeitkomplexität der DFS ist $\mathcal O(|V| + |E|)$, der Platzbedarf ist $\Theta(|V|)$.

\subsection{Zusammenhangskomponenten in ungerichteten Graphen}

Was tun wir, wenn wir alle Zusammenhangskomponenten (SCCs) eines ungerichteten Graphen bestimmen möchten?

\begin{itemize}
\item Konstruktion des dazugehörigen symmetrischen, gerichteten Graphen
\item Start der Tiefensuche bei einem beliebigen Knoten
\item Suche aller anderen Knoten (und aller Kanten) in der selben Komponente mit DFS
\item Wenn es weitere, unbesuchte Knoten gibt, Wiederholung des Verfahrens mit diesen
\end{itemize}

\subsection{Zusammenhangskomponenten in gerichteten Graphen}

\textit{Wiederholung (7.2):} Es sei $G$ ein gerichteter Graph. $G$ heißt stark zusammenhängend, wenn jeder Knoten von jedem anderen Knoten aus erreichbar ist. Eine starke Zusammenhangskomponente von $G$ ist ein maximaler stark zusammenhängender Teilgraph von $G$. Der transponierte Graph von $G$ ist $G^T$, hier ist die Richtung aller Kanten von $G$ umgedreht.

Die starken Zusammenhangskomponenten eines gerichteten Graphen induzieren den dazugehörigen \textbf{Kondensationsgraphen}. Diesen bilden wir, indem wir jede Zusammenhangskomponente quasi zu einem Knoten zusammenfassen und die Kanten zwischen den Zusammenhangskomponenten übernehmen. Der Kondensationsgraph $G\downarrow$ ist azyklisch.

Aus der Vorlesung stammt die folgende Definition für den Kondensationsgraphen, die zunächst wegen der Definition von $V'$ verwirrt: Es sei $G = (V, E)$ ein gerichteter Graph mit $k$ starken Zusammenhangskomponenten $S_i = (V_i, E_i)$ für $1 \leq i \leq k$. Der Kondensationsgraph $G\downarrow$ $= (V', E')$ ist dann definiert durch $V' = \{V_1, ..., V_k\}$ (es gibt für jede Zusammenhangskomponente einen Zustand, der quasi nach einer Menge benannt ist). Es ist $(V_i, V_j) \in E'$ gdw. $i \neq j$ und $\exists(v, w) \in E : v \in V_i$ $\land$ $w \in V_j$.

Die starken Komponenten von $G$ und $G^T$ sind die gleichen. Die Kondensation und die Transposition kommutieren: $(G\downarrow)^T = (G^T)\downarrow$.

\subsection{Erreichbarkeitsanalysen}

Die Tiefensuche wurde bereits vor Jahrhunderten formal beschrieben als ein Verfahren zum Durchqueren von Labyrinthen. Ein Labyrinth lässt sich dabei als Graph darstellen: Die Stellen, an denen mehr als ein Weg gewählt werden kann, werden als Knoten dargestellt. Die Kanten sind die Wege zwischen den Knoten. Der Eingang ist ein spezieller Knoten \textit{start}, das Ziel ein spezieller Knoten \textit{target}.

Das Problem (die Suche nach einem Weg von einem beliebigen Knoten zum Ausgang) lässt sich mit einer Tiefensuche lösen.

\subsection{Kosaraju-Sharir-Algorithmus}

Der Kosaraju-Sharir-Algorithmus findet starke Komponenten in einem gerichteten Graphen in zwei Phasen:

\begin{enumerate}
\item Führe DFS auf G durch. Speichere jeden Knoten beim Abschließen (d. h. wenn der Knoten BLACK gefärbt wird) auf einem Stack $S$.
\item Färbe alle Knoten wieder auf WHITE zurück.
\item Wiederhole, solange $S$ noch weiße Knoten enthält. Wähle den obersten noch weißen Knoten $v$ vom Stack $S$. Führe DFS mit Startknoten $v$ auf transponiertem Graphen $G^T$ aus. Speichere für jeden besuchten Knoten diesen \textit{Leiterknoten} $v$ als Repräsentanten seiner SCC.
\end{enumerate}

Ein Knoten $v$ heißt \enquote{Leiter} (lt. Vorlesung, besser \enquote{Anführer}, da engl. leader lt. VL), wenn er als letzter Knoten in seiner SCC bei der DFS BLACK gefärbt wird.

Die Worst-Zeit-Komplexität des Kosaraju-Sharir-Algorithmus ist $\Theta(|V| + |E|)$, die Speicherkomplexität ist $\Theta(|V|)$.

\subsection{Gerichtete azyklische Graphen (DAGs)}

Gerichtete kreisfreie Graphen (directed acyclic graph, DAG) sind eine wichtige Klasse von Graphen. Viele Probleme lassen sich mit Hilfe von DAGs formulieren und haben auf DAGs eine niedrigere Komplexität als auf Digraphen. Ein DAG hat eine \textbf{partielle Ordnung} $<$ auf den Knoten.

Eine Kante $(v, w)$ besagt $v < w$. Da eine partielle Ordnung nach der Vorlesung \textit{Diskrete Strukturen} transitiv und anti-symmetrisch ist, kann sie keine Kreise enthalten.

\subsection{Topologisches Sortieren}

Es sei $G = (V, E)$ ein gerichteter Graph mit $n$ Knoten. Eine topologische Ordnung von $G$ ist eine Nummerierung der Knoten. topo$: V \rightarrow \{1, ..., n\}$, sodass $topo(v) > topo(w)$ für jede Kante $(v, w) \in E$ gilt. Eine topologische Ordnung ist die Einbettung einer partiellen Ordnung in eine totale (lineare) Ordnung.

Für einen Digraphen G mit einem Kreis existiert keine topologische Ordnung. Jeder DAG hat mindestens eine topologische Ordnung.

Eine topologische Ordnung kann in $\Theta(|V| + |E|)$ bestimmt werden.

\subsection{Gewichtete Graphen}

Ein knotengewichteter Graph $G$ ist ein Tripel $(V, E, w)$, wobei $(V, E)$ ein Graph ist. Zusätzlich ist $w : V \rightarrow \mathbb{R}$ die Gewichtsfunktion und $w(v)$ mit $v \in V$ das Gewicht des Knotens $v$.

Ein kantengewichteter Graph $G$ ist ein Tripel $(V, E, w)$, wobei $(V, E)$ ein Graph ist. Zusätzlich ist $w : E \rightarrow \mathbb{R}$ die Gewichtsfunktion und $w(e)$ mit $e \in V$ das Gewicht der Kante $e$.

Wir können einen knotengewichteten Graphen $(V, E, w)$ einfach in einen kantengewichteten Graphen $(V, E, w')$ überführen, indem alle von einem Knoten $v$ ausgehenden Kanten $e = (v, -) \in E$ das Gewicht $w'(e) = w(v)$ erhalten.\\

Auch gewichtete Graphen können wir durch eine Adjazenzliste oder -matrix darstellen. Bei knoten-gewichteten Graphen speichern wir die Zusatzinformation zu den Knoten üblicherweise in einem weiteren Array. Kantengewichte können bei der Matrixdarstellung direkt in der Matrix gespeichert werden (ein besonderer Wert, etwa $\infty$, besagt, dass keine Kante existiert).

\subsection{Kritische-Pfad-Problem}

Als Gewicht eines Pfades bezeichnen wir die Summe der Kantengewichte der besuchten Kanten bei kantengewichteten Graphen und die Summe der Knotengewichte der besuchten Knoten bei knotengewichteten Graphen. Wir sollen nun den längsten Pfad in einem gewichteten DAG finden - das ist das Kritische-Pfad-Problem.

\subsection{Eulersche Kreise}

Ein eulerscher Kreis in einem Graphen $G = (V, E)$ ist ein Kreis, der jede Kante genau einmal durchläuft. 

Ein ungerichteter Graph $G$ besitzt einen eulerschen Kreis gdw. jeder Knoten einen geraden Grad hat und wenn $G$ zusammenhängend ist.

Ein gerichteter Graph $G$ besitzt einen eulerschen Kreis gdw. der Eingangsgrad jedes Knotens dem Ausgangsgrad entspricht und $G$ (stark oder schwach) zusammenhängend ist.

Wir können einen Eulerkreis in einem Graphen $G = (V, E)$ in $\mathcal O(|V| + |E|)$ Zeit berechnen.

\section{Minimale Spannbäume}

\subsection{Spannbäume}

Ein Spannbaum eines ungerichteten, zusammenhängenden Graphen $G = (V, E)$ ist ein Teilgraph von $G$, der ein ungerichteter Baum ist und alle Knoten von $G$ enthält. Übrigens ist ein Baum maximal kreisfrei und minimal zusammenhängend. Fügt man eine Kante zu einem Baum hinzu, so entsteht ein Kreis. Löscht man eine Kante eines Baumes, so erhält man zwei Zusammenhangskomponenten.

Der vollständige Graph mit $n$ Knoten hat $n^{n-2}$ Spannbäume.

\subsection{Minimale Spannbäume}

Zuerst definieren wir das Gewicht $w(G')$ eines Teilgraphens $G' = (V', E')$ des gewichteten Graphen $G$. Es ist $w(G') = \sum_{e \in E'} w(e)$. Ein Spannbaum des Graphen $G$ mit minimalem Gewicht heißt minimaler Spannbaum (minimum spanning tree, MST) von $G$.

Ein Spannbaum von $G$ kann, muss aber nicht eindeutig sein.

\subsection{Prüfer-Code}

Für Bäume gibt es einen sogenannten \textit{Prüfer-Code}, in diesen können wir einen Baum übersetzen.

Um einen Baum in einen Prüfer-Code zu übersetzen, müssen wir iterativ das \textbf{Blatt} $x$ mit kleinster Nummer bestimmen, $x$ zusammen mit der inzidenten Kante $(x, y)$ löschen und $y$ im Tupel speichern. Fertig sind wir, wenn nur noch eine Kante übrig ist. Übrigens: Ein Knoten mit Grad $d$ kommt $(d-1)$-mal im Prüfer-Code vor.

\subsection{Prim und Kruskal}

Die Algorithmen von Prim und Kruskal finden für einen gewichteten zusammenhängenden Graphen $G$ mit $n$ Knoten einen minimalen Spannbaum von $G$.\\

\textbf{Kruskal's Algorithmus}

\begin{enumerate}
\item Wähle eine billigste unmarkierte Kante.
\item Markiere sie, falls sie keinen Kreis mit anderen markierten Kanten schließt.
\item Wiederhole, bis $n-1$ Kanten markiert sind.
\end{enumerate}

\textbf{Prim's Algorithmus}

\begin{enumerate}
\item Wähle einen Startknoten.
\item Markiere die billigste vom bereits konstruierten Baum ausgehende Kante, die keinen Kreis mit anderen markierten Kanten schließt.
\item Wiederhole Schritt 2 bis $n-1$ Kanten markiert sind.
\end{enumerate}

\section{Kürzeste Pfade}

Nachfolgend geht es um das Kürzeste-Pfade-Problem. Ein Beispiel: Wir haben eine Straßenkarte, auf der der Abstand zwischen jedem Paar von benachbarten Kreuzungen eingezeichnet ist, eine Startkreuzung $s$ und eine Zielkreuzung $t$. Wie finden wir nun algorithmisch den kürzesten Weg von $s$ nach $t$?

\subsection{Notation}

Wir arbeiten mit einem kantengewichteten Graphen $G = (V, E, W)$. Ein kürzester Pfad vom Knoten $s \in V$ zum Knoten $v \in V$ ist ein Pfag von $s$ nach $v$ mit minimalem Gewicht.

Wir definieren $\delta: (V \times V) \rightarrow (\mathbb{R} \cup \{\infty\})$. Es ist $\delta(s, v)$ das Gewicht des kürzesten Pfades von $s$ nach $v$ und $\delta(s, v) = \infty$, falls $v$ von $s$ nicht erreichbar ist.

\subsection{Problemvarianten}

Es gibt viele verschiedene Varianten des Kürzeste-Pfade-Problems:

Kürzeste Pfade von einem Startknoten $s$ zu allen anderen Knoten sind Single-Source Shortest Paths (SSSP). Kürzeste Pfade von allen Knoten zu einem Zielknoten $t$ lässt sich auf SSSP zurückführen.

Für den kürzesten Pfad für ein festes Knotenpaar $(u, v)$ ist kein Algorithmus bekannt, der asymptotisch schneller ist als der beste SSSP-Algorithmus.

Die kürzesten Pfade für alle Knotenpaare ermitteln wir mit All-Pairs Shortest Paths (APSP).

\subsection{Bellman-Ford-Algorithmus}

Der Bellman-Ford-Algorithmus berechnet die kürzesten Pfade von einem einzigen Startpunkt aus, ist also ein SSSP-Algorithmus. Dabei erlaubt der Algorithmus auch negative Kantengewichte und kann negative Zyklen feststellen, die vom Startknoten aus erreichbar sind (diese sind problematisch, da man durch unendliches Durchlaufen durch diese Knoten immer kürzere Strecken erreichen und somit keine kürzeste Strecken finden kann) - dann gibt es keine Lösung und der Algorithmus terminiert, sobald ein negativer Kreis gefunden wurde.

Initial ist für den Startknoten ($s$) $dist[s] = 0$ und für alle anderen Knoten ($v \in V \setminus \{s\}$) $dist[v] = \infty$. Wenn es eine Lösung gibt (also kein negativer Kreis vorhanden ist), verbessert der Algorithmus für jeden Knoten $v$ eine obere Grenze $dist[v]$ für $\delta(s, v)$, bis das Minimum gefunden wurde. Dazu wird für alle Kanten $(v, w) \in E$ iterativ geprüft, ob das bisher bekannte Gewicht $dist[w]$ größer als $dist[v] + W(v, w)$ ist - ist dies der Fall, wird $dist[w]$ auf den neuen Wert verbessert.

Wir sind fertig, wenn keine Verbesserungen mehr möglich sind. Gibt es nach $|V|-1$ Wiederholungen noch Verbesserungen, so existiert ein negativer Kreis. Ansonsten gilt $dist[v] = \delta(s, v)$ für alle Knoten $v \in V$.

Der Algorithmus speichert zusätzlich die Vorgängerknoten entlang eines kürzesten Pfades im Array $prev$, sodass sich so nach Terminierung die kürzesten Pfade rekonstruieren lassen.

Die Zeitkomplexität des Algorithmus ist gegeben durch $\mathcal O(|V|\cdot{}|E|)$.

\subsection{Dijkstra-Algorithmus}

Dijkstra arbeitet im Gegensatz zu Bellman und Ford nur mit nicht-negativen Kantengewichten. Es gilt also $\forall (v, w) \in E : W(v, w) \geq 0$. Kürzeste Pfade enthalten damit keine Zyklen.

Wir beginnen mit unserem Startknoten $s$, dieser hat $dist[s] = 0$. Alle anderen Knoten $v \in V \setminus \{s\}$ haben $dist[v] = \infty$. Der Startknoten wird schwarz gefärbt, er ist Teil unseres (Hilfs-)Baums. Wir betrachten nun alle Randkanten $(s, v) \in E$ und suchen die Kante mit dem geringsten Kantengewicht. Sodann fügen wir den neuen Zielknoten $v$ zum Baum hinzu, indem wir ihn ebenfalls schwarz färben. Wir betrachten nun iterativ die Randkanten unseres Baums und suchen immer wieder die billigste Kante (dabei muss auf die Summe geachtet werden, es dürfen ferner keine Zyklen entstehen). Entsprechend füllen wir $dist[v]$ für $v \in V$, bis alle Knoten im Baum sind.

Der Dijkstra-Algorithmus findet die kürzesten Wege mit zunehmenden Abstand zur Quelle $s$, die Implementierung ist ähnlich zum Spannbaum-Algorithmus von Prim (8.4). Die Zeitkomplexität im Worst-Case ist $\in \Theta(|V|^2)$, ferner ist die untere Schranke $\Omega(|E|)$. Wir haben eine Platzkomplexität von $\mathcal O(|V|)$.

\subsection{All-Pairs Shortest Paths}

Wir betrachten gewichtete, gerichtete Graphen $G = (V, E, W)$. Dabei sind negative Gewichte zwar erlaubt, aber keine Kreise mit negativen Gewicht. Nicht vorhandene Kanten haben das Gewicht $W(-,=) = \infty$. Wir möchten nun für jedes Paar $(i, j)$ das Gewicht $D[i,j]$ des kürzesten Pfades berechnen.

Man kann nun einen SSSP-Algorithmus (z.B. Bellman-Ford) $|V|$-mal anwenden, das führt allerdings zu einer Worst-Case-Zeitkomplexität von $\mathcal O(|V|^4)$. Im Folgenden werden wir effizientere Algorithmen für das APSP-Problem kennenlernen.

\subsection{Binäre Relationen}

Eine (binäre) Relation $R$ über einer Menge $S$ ist eine Teilmenge $R \subseteq S \times S = S^2$.

Aus der Vorlesung \textit{Diskrete Strukturen} kennen wir die Reflexivität ($\forall u \in S : (u, u) \in R$) und die Transitivität ($(u, v) \in R \land (v, w) \in R => (u, w) \in R$). Die transitive Hülle $R^*$ einer Relation $R$ ist die kleinste Erweiterung (Obermenge) $R \subseteq R^* \subseteq S^2$, sodass $R^*$ reflexiv und transitiv ist.

Für einen Graphen mit der Menge $S = V$ und der Relation $R = E$ gilt: Es ist $(u, v) \in R^*$ gdw. ein Pfad von $u$ nach $v$ existiert.

\subsection{Warshall-Algorithmus}

Der Warshall-Algorithmus ist die Basis für den Floyd-Algorithmus. Er berechnet die transitive Hülle eines Graphen. Dazu wird zuerst die Reflexivität erstellt - im Graphen wird so bei jedem Knoten eine Schleife hinzugefügt (sofern noch keine vorhanden ist), in der Adjazenzmatrix wird die Diagonale mit Einsen gefüllt.

Dann schauen wir für jeden Knoten einzeln, welche Knoten direkt von diesem aus erreichbar sind. Das machen wir rekursiv für die erreichbaren Knoten und fügen die entsprechenden direkten Kanten vom ursprünglichen Knoten zum über den neuen Knoten erreichbaren Knoten hinzu bzw. schreiben die entsprechende Eins in die Matrix.

Nachdem wir das für alle Knoten gemacht haben (in der Regel machen wir das in aufsteigender Reihenfolge der Knotennummerierung), haben wir die transitive Hülle als Graph bzw. ihre Adjazenzmatrix.

Die Zeitkomplexität das Warshall-Algorithmus ist $\Theta(|V|^3)$, die Platzkomplexität ist $\Theta(|V|^2)$.

\subsection{Floyd-Algorithmus}

\textit{Es gelten die Voraussetzungen aus 9.5.} Der Floyd-Algorithmus greift auf den Warshall-Algorithmus zurück, allerdings werden hier die Distanzen berücksichtigt (statt der Berechnung der linearen Hülle). Deshalb wird er auch als Floyd-Warshall-Algorithmus bezeichnet.

Das Vorgehen ist wie bei Warshall, allerdings nutzen wir folgende Rekursionsgleichung für die Distanzen: Es ist $d_{ij}^{(0)} = W(i, j)$ und $d_{ij}^{(k)} = min \{d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}\}$ für $k > 0$.

\section{Matching}

\subsection{Definition}

Es sei $G = (V, E)$ ein ungerichteter Graph. Eine Kantenmenge $M \subseteq E$ heißt Matching, falls je zwei verschiedene Kanten in $M$ disjunkt sind (für alle $e, e' \in M$ mit $e \neq e'$ gilt $e \cap e' = \emptyset$). Ein Knoten $v \in V$ wird von einem Matching $M$ überdeckt, falls $v \in e$ für eine Kante $e \in M$ gilt.

Wir bezeichnen einen Graphen $G = (V, E)$ als \textbf{bipartit}, falls es eine Partition $V = X \cup Y$ der Knotenmenge gibt, sodass $E \subseteq X \times Y$.

Für einen ungerichteten Graphen $G = (V, E)$ suchen wir oft ein \textbf{Matching} $M \subseteq E$ mit maximaler Kardinalität. Ist der Graph $G$ bipartit, bezeichnen wir das Matching auch als \textbf{bipartites Matching}.

\subsection{Algorithmus für bipartites Matching}

In der Vorlesung wurde ein Algorithmus für bipartites Matching entwickelt. Dabei beginnen wir mit dem leeren Matching $M = \emptyset$. Das Matching lassen wir Schritt für Schritt wachsen, wobei jeder Schritt die Kardinalität $|M|$ um $1$ erhöht. Wenn wir stecken bleiben, ist $M$ ein Matching maximaler Kardinalität.

VL18 (11.08.2017)

\section{Maximaler Fluss}

Wir haben ein Netzwerk mit einer Quelle und einer Senke, die über mehrere Wege miteinander verbunden sind. Diese Wege haben unterschiedliche Kapazitäten (wie zum Beispiel Straßen). Uns interessiert der maximale Fluss, also die maximale Rate, mit der Material von der Quelle bis zur Senke transportiert werden kann, ohne die Kapazitätsbeschränkungen der Straßen zu verletzen.

\subsection{Flussnetzwerke}

Zur Formalisierung des Problems verwenden wir ein Flussnetzwerk. Das ist ein gerichteter Graph $G = (V, E, c)$ mit der Kapazitätsfunktion $c : V \times V \rightarrow \mathbb{R}_{\geq 0}$, einer Quelle $s$ und einer Senke $t$. Dabei liegt jeder Knoten $v \in V$ auf einem Pfad von der Quelle $s$ zur Senke $t$.

Ein Fluss ist eine Funktion $f : V \times V \rightarrow \mathbb{R}$, dabei ist $f(u, v)$ der Fluss vom Knoten $u$ zum Knoten $v$. Es ist für $u, v \in V$ immer $f(u, v) \leq c(u, v)$ und $f(u, v) = -f(v, u)$.  Ferner gilt für $u \in V \setminus \{s, t\}$ immer $\sum_{v \in V}$ $f(u, v) = 0$.

Der Wert $|f|$ eines Flusses $f$ ist der Gesamtfluss aus der Quelle $s$.

Ein Flussnetzwerk mit mehreren Quellen und Senken lässt sich auf ein Flussnetzwerk mit einer Quelle und einer Senke zurückführen, indem wir eine \enquote{Superquelle} und eine \enquote{Supersenke} einführen, die wir mit den einzelnen Quellen bzw. Senken verbinden.

Der Fluss zwischen zwei Knoten lässt sich leicht auf den Fluss zwischen einem Knoten und einer Knotenmenge oder sogar zwischen zwei Knotenmengen erweitern. Es ist für $x \in V, Y \subseteq V$ stets $f(x, Y) = \sum_{y \in Y}$ $f(x, y)$ und für $X, Y \subseteq V$ stets $f(X, Y) = \sum_{x \in X} \sum_{y \in Y}$ $f(x, y)$. Für $X \subseteq V$ gilt übrigens $f(X, X) = 0$.

Wir nennen ein Flussproblem \textbf{integral}, wenn alle Kapazitäten ganzzahlig sind.

\subsection{Restnetzwerke}

Ein Restnetzwerk beschreibt, welche Kapazitäten in einem gegebener Flussnetzwerk mit einem bestimmten Fluss noch vorhanden sind. Sei das Flussnetzwerk $G = (V, E, c)$ und der Fluss $f$ gegeben. Dann ist $G_f = (V, E_f, c_f)$ das Restnetzwerk (auch Residualnetzwerk) zu $G$ und $f$ mit $c_f(u, v) = c(u, v) - f(u, v)$ und $E_f = \{(u, v) \in V \times V | c_f(u, v) > 0\}$.

Damit ist $c_f(u, v)$ die Restkapazität von $(u, v)$ in $G$ zum Fluss $f$ und $E_f$ sind alle Kanten, die noch mehr Fluss aufnehmen können.

Es muss unbedingt beachtet werden, dass die Restkapazität in beide Richtungen zwischen den Knoten beachtet wird (negativer Fluss!). Daher gilt stets $|E_f| \leq 2\cdot|E|$.

Ein $s$-$t$-Pfad $p$ im Restnetzwerk $G_f$ heißt augmentierender Pfad (vergrößernder Pfad), da über diesen der Fluss im Flussnetzwerk noch erhöht werden kann. Die Restkapazität des Pfades $p$ ist die kleinste Kapazität der Kanten in diesem Pfad.

\subsection{Ford-Fulkerson-Theorem}

Wir haben einen Fluss $f$ in $G$ und einen Fluss $f'$ im Restnetzwerk $G_f$. Die Hauptidee der Ford-Fulkerson-Theorem ist es, zu versuchen, den Fluss $f$ um den Fluss $f'$ zu ergänzen.

Dazu definieren wir zunächst die \textbf{Flusssumme}. Es ist $f_1 + f_2$ trivial definiert durch $(f_1+f_2)(u,v) = f_1(u,v)+f_2(u,v)$.

Nach dem Ford-Folkerson-Theorem ist $f + f'$ ein Fluss in $G$ mit dem Wert $|f + f'|$.

\subsection{Ford-Fulkerson-Methode}

Bei der Ford-Fulkerson-Methode suchen wir zunächst einen beliebigen Pfad $p$ von $s$ nach $t$. Nun setzen wir den Fluss der Kanten in $p$ auf die minimale Kapazität in $p$ (wenn eine Kante eine Kapazität von $5$ setzen wir den Fluss aller Kanten in $p$ auf $5$). Sodann suchen wir uns einen neuen (wieder beliebigen) Pfad $p'$ von $s$ nach $t$ aus Kanten mit \textbf{freier} Kapazität (die Kante, die soeben das Minimum gestellt hat, wird also nicht mehr verwendet). Es wird dann der Fluss entsprechend der minimalen Restkapazität auf diesem Pfad angepasst. Das wiederholen wir, bis er keinen Pfad mit freien Kapazitäten mehr gibt.

Das entspricht einer Anwendung des Ford-Fulkerson-Theorems: Solange es einen augmentierenden Pfad $p$ gibt, augmentieren wir $f$ entlang $p$.

Sei $f^*$ der durch die Ford-Fulkerson-Methode bestimmte Fluss zu einem integralen Flussproblem, so benötigt die Methode $|f^*|$ Iterationen und es ergibt sich eine Laugzeit von $\mathcal O(|E| \cdot |F^*|)$.

\subsection{Schnitte in Flussnetzwerken}

Ein Schnitt $(S, T)$ in einem Flussnetzwerk $G= (V, E, c)$ ist eine Partition $S \cup T = V, S \cap T = \emptyset$ mit $s \in S$ und $t \in T$. Wenn $f$ ein Fluss in $G$ ist, dann ist $f(S, T)$ der Fluss über $(S, T)$. Die Kapazität von $(S, T)$ ist $c(S, T)$. Ein minimaler Schnitt ist ein Schnitt mit minimaler Kapazität. Für den Fluss über einen Schnitt gilt stets $f(S, T) = |f|$.

\subsection{Max-flow Min-cut Theorem}

Das Max-flow min-cut Theorem besagt für einen Fluss $f$ im Flussnetzwerk $G=(V,E,c)$, dass folgende Sätze äquivalent sind:

\begin{enumerate}
\item $f$ ist ein maximaler Fluss.
\item Im Restnetzwerk $G_f$ gibt es keinen augmentierenden Pfad.
\item $|f| = c(S, T)$ für einen Schnitt $(S, T)$, d.h. $(S, T)$ ist minimal.
\end{enumerate}

Damit ist die Kapazität eines minimalen Schnittes (min cut) gleich dem Wert eines maximalen Flusses (max flow). Mit diesem Theorem lässt sich auch die Korrektheit der Ford-Fulkerson-Methode zeigen.

\subsection{Edmonds-Karp-Algorithmus}

Der Edmonds-Karp-Algorithmus übernimmt die Ford-Fulkerson-Methode, wählt den augmentierenden Pfad aber nicht beliebig, sondern nutzt zur Bestimmung der Pfade eine Breitensuche. Damit präferiert der Algorithmus kürzere Pfade gegenüber längeren - er erweitert den Fluss stets entlang kürzester Pfade. Die Laufzeit liegt in $\mathcal O(|V|\cdot|E|^2)$.

\section{Dynamische Programmierung}

\subsection{Memoization}

Der Hauptansatz von dynamischer Programmierung ist, bereits berechnete Werte nicht noch einmal zu berechnen. Besonders wichtig ist das bei Rekursionsgleichungen (wie etwa bei Fibonacci), weil man hier oft den gleichen Wert sehr oft verwenden muss.

Deshalb speichern wir einmal berechneten Werte. Das kostet Speicher, kann aber zu einer enormen Reduzierung der Zeitkomplexität führen. Bei jedem Funktionsaufruf prüfen wir nun, ob das Ergebnis bereits berechnet wurde (im Cache ist) - ist dies der Fall, holen wir den Wert aus dem Speicher. Ansonsten müssen wir den Wert berechnen, zusätzlich speichern wir das Ergebnis im Cache.

Memoization hilft, wenn die Teilprobleme überlappen. Bei den Fibonacci-Zahlen sinkt die Zeitkomplexität von $\Theta(2^n)$ auf $\Theta(n)$, was natürlich enorm ist. Dafür wächst der Platzbedarf aber auf $\Theta(n)$. Eine Verbesserung ergibt sich bei den Fibonacci-Zahlen, indem man erkennt, dass nur jeweils die zwei letzten Werte benötigt werden (in-place).

\subsection{The principle of optimality}

Es sei $L$ die optimale Lösung für ein Problem $P$. Es sei $Q$ ein Teilproblem von $P$. Dann ist die Restriktion von $L$ auf $Q$ eine optimale Lösung für das Teilproblem $Q$.

\subsection{Ketten von Matrixmultiplikation}

Bei einer einfachen Matrixmultiplikation benötigen wir $i \cdot j \cdot k$ Fließkomma-Multiplikationen (Zeilen erste Matrix * Spalten erste Matrix * Spalten zweite Matrix). Wir betrachten nun die Multiplikation mehrerer Matrizen $M = A_1 \cdot A_2 \cdot ... \cdot A_n$ (wir gehen davon aus, dass die Matrizen jeweils miteinander kompatibel bzgl. der Matrixmultiplikation sind). Solche Ketten lassen sich wegen der Assoziativität der Matrixmultiplikation in beliebiger Reihenfolge berechnen bzw. klammern - darin liegt der Trick.

Wenn wir beispielsweise drei Matrizen multiplizieren sollen, können wir $(A_1 \cdot A_2) \cdot A_3$ oder $A_1 \cdot (A_2 \cdot A_3)$ berechnen. Das kann ein gravierender Unterschied sein: Sei $A_1 \in \mathbb{R}^{10 \times 100}$, $A_2 \in \mathbb{R}^{100 \times 5}$ und $A_3 \in \mathbb{R}^{5 \times 50}$. Dann müssen wir bei der ersten Klammerung 7.500 Multiplikationen durchführen - bei der zweiten Klammerung benötigen wir allerdings enorme 75.000 Multiplikationen.

Die Problemstellung ist nun also, für eine Kette von Matrizen $A_1, A_2, ..., A_n$ mit den Dimensionen $d_0 \times d_1, d_1 \times d_2, ..., d_{n-1} \times d_n$ eine Klammerung so zu finden, dass die Anzahl die Fließkomma-Multiplikationen minimal ist - wir möchten die Berechnung ja möglichst effizient durchführen.

Es sei nun $P(n)$ die Anzahl der möglichen Klammerungen für $n$ Matrizen. Wir können nun bestimmen, dass $P(n) = \sum_{k = 1}^{n - 1} P(k) \cdot P(n-k) \in \Omega(2^n)$ und $P(1) = 1$ gilt. Allerdings benötigen wir eine Rekursionsgleichung für die minimale Anzahl an Multiplikationen.

Sei nun $m[i,j]$ die minimale Anzahl Multiplikationen für die Teilkette $A_i \cdot ... \cdot A_j$. Offenbar ist $m[i,i] = 0$ für alle $0 < i \leq n$. Die Dimension einer Teilkette ist $d_{i-1} \times d_j$. Durch das Teilen der Kette an der Position $k$ ergibt sich: $m[i,j] = m[i,k] + d_{i-1} \cdot d_k \cdot d_j + m[k+1,j]$. Wir suchen dabei das optimale $k$, also ist $m[i,j] = 0$ für $i = j$ und für $i<j$ ist $min[i,j]=min_{i \leq k < j}(m[i,k]+m[k+1,j]+d_{i-1} \cdot d_k \cdot d_j)$.

Wie bei Fibonacci werden Teilprobleme wieder mehrfach verwendet - z.B. benötigen wir $m[0,1]$ für $m[0,2], m[0,3], ..., m[0,n]$. Deswegen kann DP zum Einsatz kommen. Wir berechnen die Matrix von unten nach oben und von links nach rechts.

Die Zeitkomplexität war exponentiell, nun ist sie $\Theta(n^3)$. Für die naive Variante wurde kein Platz benötigt, nun haben wir einen Platzbedarf von $\Theta(n^2)$.

\subsection{Das Rucksackproblem}

Gegeben sei ein Rucksack mit maximaler Tragkraft $M$ sowie $n$ Gegenstände, die sowohl ein Gewicht als auch einen Wert haben. Wir wollen nun möglichst viel Wert mitnehmen, ohne den Rucksack zu überladen. Ein erster Ansatz ist, die Gegenstände nach dem Gewicht-Wert-Verhältnis absteigend zu sortieren und dann solange Gegenstände einzupacken, bis die maximale Tragkraft überschritten werden würde. Allerdings ist dieser Algorithmus nicht optimal, weil eventuell noch Gegenstände reinpassen würden.

Wir überlegen uns nun eine Rekursionsgleichung. Diese basiert auf der Überlegung, ob wir den $n$-ten Gegenstand mitnehmen (können) oder nicht. Es sei nun $C[i,j]$ der maximale Wert des Rucksacks mit Tragkraft $j$, wenn man nur die Gegenstände $\{0, ..., i-1\}$ berücksichtigt. Daraus ergibt sich $C[i,j] = 0$ für $i = 0$, $C[i,j] = -\infty$ für $j < 0$ und ansonsten $C[i,j]=max(C[i-1,j], c_{i-1}+C[i-1, j-w_{i-1}])$. Damit ist $c_{max} = C[n,M]$.

Wir lösen die Rekursionsgleichung nun buttom-up und kommen auf die gleiche Zeit- und Platzkomplexität $\Theta(n\cdot M)$.

\section{Greedy-Algorithmen}

VL21 (12.08.2017)

\section{Algorithmische Geometrie}

VL22 (12.08.2017)

\section{Abkürzungsverzeichnis}

12.08.2017

\end{document}
